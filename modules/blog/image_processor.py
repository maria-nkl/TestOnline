# modules/blog/image_processor.py
import os
import cv2
import numpy as np
import torch
import torch.nn as nn
from torchvision import transforms
from PIL import Image
from torchvision.models import resnet18
from django.conf import settings
import logging
from typing import Tuple, List, Dict, Optional  # –î–æ–±–∞–≤–ª—è–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –∏–º–ø–æ—Ä—Ç—ã —Ç–∏–ø–æ–≤

logger = logging.getLogger(__name__)

class PredictionConfig:
    img_size = 64  # –†–∞–∑–º–µ—Ä –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–π –¥–ª—è –º–æ–¥–µ–ª–∏

class ImageProcessor:
    def __init__(self):
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        self.model = self._load_model()
        self.base_output_dir = os.path.join(settings.MEDIA_ROOT, 'processed_images')
        
        # –°–æ–∑–¥–∞–µ–º –Ω–µ–æ–±—Ö–æ–¥–∏–º—ã–µ –¥–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
        os.makedirs(self.base_output_dir, exist_ok=True)

    class CustomResNet(nn.Module):
        def __init__(self):
            super().__init__()
            self.resnet = resnet18(pretrained=False)
            self.resnet.fc = nn.Sequential(
                nn.Linear(self.resnet.fc.in_features, 512),
                nn.ReLU(),
                nn.Dropout(0.5),
                nn.Linear(512, 1)
            )

        def forward(self, x):
            return torch.sigmoid(self.resnet(x))
    
    def _load_model(self):
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–æ –æ–±—É—á–µ–Ω–Ω—É—é –º–æ–¥–µ–ª—å"""
        model = self.CustomResNet().to(self.device)
        model_path = os.path.join(settings.BASE_DIR, 'best_model.pth')
        
        if not os.path.exists(model_path):
            raise FileNotFoundError(f"–§–∞–π–ª –º–æ–¥–µ–ª–∏ –Ω–µ –Ω–∞–π–¥–µ–Ω: {model_path}")
        
        model.load_state_dict(torch.load(model_path, map_location=self.device))
        model.eval()
        return model
    
    def format_processing_results(self, squares_data):
        """
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç HTML-–æ—Ç—á—ë—Ç —Å —É–∑–∫–∏–º —Å—Ç–æ–ª–±—Ü–æ–º –æ—Ç–º–µ—Ç–æ–∫
        """
        result = """
        <div style='font-family: monospace; max-width: 500px;'>
        <h3 style='color: #2c3e50; margin-bottom: 10px;'>üìä –†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ —à–∞–±–ª–æ–Ω–æ–≤</h3>
        """
        
        for template, questions in squares_data.items():
            result += f"""
            <div style='margin-bottom: 25px;'>
                <h4 style='color: #3498db; margin: 5px 0;'>üìã –®–∞–±–ª–æ–Ω: {template}</h4>
                <table style='width: 100%; border-collapse: collapse; font-size: 14px;'>
                    <thead>
                        <tr style='background-color: #f8f9fa;'>
                            <th style='padding: 6px; border: 1px solid #dee2e6; text-align: center; width: 30%;'>–í–æ–ø—Ä–æ—Å ‚Ññ</th>
                            <th style='padding: 6px; border: 1px solid #dee2e6; text-align: center; width: 30%;'>–û—Ç–º–µ—Ç–∫–∏</th>
                        </tr>
                    </thead>
                    <tbody>
            """
            
            for question, answers in sorted(questions.items(), key=lambda x: int(x[0])):
                marked = [a for a, s in sorted(answers.items()) if s == "marked"]
                marks = " ".join(f"<span style='color: #28a745;'>‚úì{a}</span>" for a in marked) if marked else "<span style='color: #dc3545;'>‚úó</span>"
                
                result += f"""
                    <tr>
                        <td style='padding: 6px; border: 1px solid #dee2e6; text-align: center;'>{question}</td>
                        <td style='padding: 6px; border: 1px solid #dee2e6; text-align: center;'>{marks}</td>
                    </tr>
                """
            
            result += """
                    </tbody>
                </table>
            </div>
            """
        
        result += "</div>"
        return result
    
    def process_uploaded_image(self, image_path: str) -> Dict:
        """
        –û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –¥–ª—è –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∑–∞–≥—Ä—É–∂–µ–Ω–Ω–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è.
        –í–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–ª–∏ –æ—à–∏–±–∫–æ–π.
        """
        try:
            # –°–æ–∑–¥–∞–µ–º —É–Ω–∏–∫–∞–ª—å–Ω—ã–µ –ø–∞–ø–∫–∏ –¥–ª—è –∫–∞–∂–¥–æ–≥–æ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            base_name = os.path.splitext(os.path.basename(image_path))[0]
            session_dir = os.path.join(self.base_output_dir, base_name)
            
            output_norm_folder = os.path.join(session_dir, 'normalized')
            output_rectangles_folder = os.path.join(session_dir, 'rectangles')
            output_squares_folder = os.path.join(session_dir, 'squares')
            
            os.makedirs(output_norm_folder, exist_ok=True)
            os.makedirs(output_rectangles_folder, exist_ok=True)
            os.makedirs(output_squares_folder, exist_ok=True)

            # 1. –ó–∞–≥—Ä—É–∑–∫–∞ –∏ –ø—Ä–µ–¥–≤–∞—Ä–∏—Ç–µ–ª—å–Ω–∞—è –æ–±—Ä–∞–±–æ—Ç–∫–∞
            image, gray = self._load_and_preprocess_image(image_path)

            # 2. –ü–æ–∏—Å–∫ –º–∞—Ä–∫–µ—Ä–æ–≤
            markers = self._find_markers(gray)
            if len(markers) != 4:
                return {"error": f"–ù–∞–π–¥–µ–Ω–æ {len(markers)} –º–∞—Ä–∫–µ—Ä–æ–≤ (—Ç—Ä–µ–±—É–µ—Ç—Å—è 4)"}

            # 3. –ù–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—è –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è
            norm_output_path = os.path.join(output_norm_folder, f"{base_name}_norm.jpg")
            normalized = self._normalize_image(image, markers, norm_output_path)
            if normalized is None:
                return {"error": "–ù–µ–≤–æ–∑–º–æ–∂–Ω–æ –≤—ã–ø–æ–ª–Ω–∏—Ç—å –Ω–æ—Ä–º–∞–ª–∏–∑–∞—Ü–∏—é"}

            # 4. –ò–∑–≤–ª–µ—á–µ–Ω–∏–µ –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–æ–≤
            rectangles = self._extract_numbered_rectangles(
                norm_output_path,
                output_rectangles_folder
            )

            # 5. –û–±—Ä–∞–±–æ—Ç–∫–∞ –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–æ–≤ –¥–ª—è –ø–æ–∏—Å–∫–∞ –∫–≤–∞–¥—Ä–∞—Ç–æ–≤
            squares_data = self._process_squares(
                output_rectangles_folder,
                output_squares_folder
            )

            # –§–æ—Ä–º–∏—Ä—É–µ–º –æ—Ç–Ω–æ—Å–∏—Ç–µ–ª—å–Ω—ã–µ –ø—É—Ç–∏ –¥–ª—è —Ö—Ä–∞–Ω–µ–Ω–∏—è –≤ –ë–î
            rel_path = lambda p: os.path.relpath(p, settings.MEDIA_ROOT)

            
            return self.format_processing_results(squares_data)
        
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è {image_path}: {str(e)}")
            return f"## ‚ùå –û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏\n\nüõë –ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞: {str(e)}"
    

    def _load_and_preprocess_image(self, image_path: str) -> Tuple[np.ndarray, np.ndarray]:
        """–ó–∞–≥—Ä—É–∂–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –ø—Ä–µ–æ–±—Ä–∞–∑—É–µ—Ç –µ–≥–æ –≤ –æ—Ç—Ç–µ–Ω–∫–∏ —Å–µ—Ä–æ–≥–æ."""
        image = cv2.imread(image_path)
        if image is None:
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {image_path}")
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        return image, gray

    def _get_marker_corners(self, contour: np.ndarray) -> np.ndarray:
        """–ù–∞—Ö–æ–¥–∏—Ç 4 —É–≥–ª–∞ –º–∞—Ä–∫–µ—Ä–∞ —Å –ø–æ–º–æ—â—å—é –∞–ø–ø—Ä–æ–∫—Å–∏–º–∞—Ü–∏–∏ –∫–æ–Ω—Ç—É—Ä–∞."""
        epsilon = 0.02 * cv2.arcLength(contour, True)
        approx = cv2.approxPolyDP(contour, epsilon, True)

        if len(approx) != 4:
            rect = cv2.minAreaRect(contour)
            box = cv2.boxPoints(rect)
            approx = np.int0(box)

        approx = approx.reshape(4, 2)
        center = approx.mean(axis=0)
        angles = np.arctan2(approx[:, 1] - center[1], approx[:, 0] - center[0])
        sorted_idx = np.argsort(angles)
        return approx[sorted_idx]

    def _find_markers(self, gray_image: np.ndarray, min_area: int = 500, max_area: int = 5000) -> List[np.ndarray]:
        """–ù–∞—Ö–æ–¥–∏—Ç —É–≥–ª–æ–≤—ã–µ –º–∞—Ä–∫–µ—Ä—ã (—á–µ—Ä–Ω—ã–µ –∫–≤–∞–¥—Ä–∞—Ç—ã) –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏."""
        thresh = cv2.adaptiveThreshold(
            gray_image, 255, cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
            cv2.THRESH_BINARY_INV, 51, 5
        )

        contours, _ = cv2.findContours(thresh, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        markers = []
        for cnt in contours:
            area = cv2.contourArea(cnt)
            if min_area < area < max_area:
                x, y, w, h = cv2.boundingRect(cnt)
                aspect_ratio = float(w) / h
                if 0.8 < aspect_ratio < 1.2:
                    corners = self._get_marker_corners(cnt)
                    markers.append(corners)

        return markers

    def _normalize_image(
            self,
            image: np.ndarray,
            markers: List[np.ndarray],
            output_path: str,
            output_size: Tuple[int, int] = (1000, 1500),
            crop_percent: int = 10,
    ) -> Optional[np.ndarray]:
        """–í—ã—Ä–∞–≤–Ω–∏–≤–∞–µ—Ç –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ –∏ –æ–±—Ä–µ–∑–∞–µ—Ç –≤–µ—Ä—Ö/–Ω–∏–∑, —á—Ç–æ–±—ã —É–¥–∞–ª–∏—Ç—å –º–∞—Ä–∫–µ—Ä—ã."""
        markers_array = np.array(markers)
        centers = np.array([np.mean(marker, axis=0) for marker in markers_array])

        sorted_indices = np.argsort(centers[:, 1])
        top_indices = sorted_indices[:2]
        bottom_indices = sorted_indices[2:]
        top_sorted = top_indices[np.argsort(centers[top_indices, 0])]
        bottom_sorted = bottom_indices[np.argsort(centers[bottom_indices, 0])[::-1]]
        final_order = np.concatenate([top_sorted, bottom_sorted])
        centers_sorted = centers[final_order]

        src_points = np.float32(centers_sorted)
        width, height = output_size
        dst_points = np.float32([[0, 0], [width, 0], [width, height], [0, height]])

        matrix = cv2.getPerspectiveTransform(src_points, dst_points)
        result = cv2.warpPerspective(image, matrix, (width, height))

        crop_pixels = int(height * crop_percent / 100)
        result = result[crop_pixels: height - crop_pixels, :]

        cv2.imwrite(output_path, result, [int(cv2.IMWRITE_JPEG_QUALITY), 90])
        return result

    def _extract_numbered_rectangles(
            self,
            input_image_path: str,
            output_folder: str,
            expected_count: int = 20,
            min_area: int = 20000,
            max_area: int = 50000,
            aspect_ratio_range: Tuple[float, float] = (4.5, 7.5)
    ) -> List[Tuple[int, int, int, int]]:
        """–ò–∑–≤–ª–µ–∫–∞–µ—Ç –ø—Ä—è–º–æ—É–≥–æ–ª—å–Ω–∏–∫–∏ —Å –ø—Ä–∞–≤–∏–ª—å–Ω–æ–π –Ω—É–º–µ—Ä–∞—Ü–∏–µ–π –¥–ª—è –¥–≤—É—Ö —Å—Ç–æ–ª–±—Ü–æ–≤."""
        image = cv2.imread(input_image_path)
        if image is None:
            raise ValueError(f"–ù–µ —É–¥–∞–ª–æ—Å—å –∑–∞–≥—Ä—É–∑–∏—Ç—å –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–µ: {input_image_path}")

        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)
        thresh = cv2.adaptiveThreshold(
            blurred, 255,
            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
            cv2.THRESH_BINARY_INV, 51, 7
        )

        kernel = np.ones((3, 3), np.uint8)
        processed = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations=2)

        contours, _ = cv2.findContours(processed, cv2.RETR_EXTERNAL, cv2.CHAIN_APPROX_SIMPLE)

        rectangles = []
        for cnt in contours:
            x, y, w, h = cv2.boundingRect(cnt)
            area = w * h
            aspect_ratio = max(w, h) / min(w, h)

            if (min_area <= area <= max_area) and (aspect_ratio_range[0] <= aspect_ratio <= aspect_ratio_range[1]):
                rectangles.append((x, y, w, h))

        if len(rectangles) >= 2:
            x_coords = [r[0] for r in rectangles]
            median_x = np.median(x_coords)

            left_col = [r for r in rectangles if r[0] < median_x]
            right_col = [r for r in rectangles if r[0] >= median_x]

            left_col.sort(key=lambda r: r[1])
            right_col.sort(key=lambda r: r[1])

            rectangles = left_col[:10] + right_col[:10]

        base_name = os.path.splitext(os.path.basename(input_image_path))[0]

        for i, (x, y, w, h) in enumerate(rectangles[:expected_count], 1):
            roi = image[y:y + h, x:x + w]
            output_path = os.path.join(output_folder, f"{base_name}_{i}.jpg")
            cv2.imwrite(output_path, roi)

        return rectangles[:expected_count]

    def _find_squares(
            self,
            image: np.ndarray,
            min_area: int = 2000,
            max_area: int = 3000
    ) -> Tuple[List[Tuple[int, int, int, int]], List[Dict]]:
        """–ù–∞—Ö–æ–¥–∏—Ç –∫–≤–∞–¥—Ä–∞—Ç—ã –Ω–∞ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏–∏."""
        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)
        blurred = cv2.GaussianBlur(gray, (5, 5), 0)

        thresh = cv2.adaptiveThreshold(
            blurred, 255,
            cv2.ADAPTIVE_THRESH_GAUSSIAN_C,
            cv2.THRESH_BINARY_INV, 31, 5
        )

        kernel = np.ones((3, 3), np.uint8)
        processed = cv2.morphologyEx(thresh, cv2.MORPH_CLOSE, kernel, iterations=2)

        contours, _ = cv2.findContours(processed, cv2.RETR_LIST, cv2.CHAIN_APPROX_SIMPLE)

        squares = []
        rejected = []

        for cnt in contours:
            epsilon = 0.02 * cv2.arcLength(cnt, True)
            approx = cv2.approxPolyDP(cnt, epsilon, True)
            x, y, w, h = cv2.boundingRect(cnt)
            area = w * h
            aspect_ratio = float(w) / h
            vertices = len(approx)

            if (vertices == 4 and min_area <= area <= max_area and 
                0.8 <= aspect_ratio <= 1.2 and cv2.isContourConvex(approx)):
                squares.append((x, y, w, h))
            else:
                rejected.append({
                    "position": (x, y, w, h),
                    "area": area,
                    "aspect_ratio": aspect_ratio,
                    "vertices": vertices
                })

        squares.sort(key=lambda s: s[0])
        return squares, rejected

    def _process_squares(self, input_folder: str, output_folder: str) -> Dict[str, Dict[int, Dict[int, str]]]:

        """–û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ—Ç –≤—Å–µ –∏–∑–æ–±—Ä–∞–∂–µ–Ω–∏—è –≤ –ø–∞–ø–∫–µ –∏ –≤–æ–∑–≤—Ä–∞—â–∞–µ—Ç —Å–ª–æ–≤–∞—Ä—å —Å —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞–º–∏"""
        results = {}
        
        for filename in sorted(os.listdir(input_folder)):
            if not filename.lower().endswith(('.png', '.jpg', '.jpeg')):
                continue

            try:
                # –ò–∑–≤–ª–µ–∫–∞–µ–º –∏–º—è —à–∞–±–ª–æ–Ω–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, Bushueva –∏–∑ Bushueva_czmgvHD_norm_1_square_2_empty.jpg)
                template_name = filename.split('_')[0]
                if template_name not in results:
                    results[template_name] = {}

                file_path = os.path.join(input_folder, filename)
                image = cv2.imread(file_path)
                if image is None:
                    continue

                squares, _ = self._find_squares(image)
                base_name = os.path.splitext(filename)[0]
                
                for i, (x, y, w, h) in enumerate(squares, 1):
                    square_img = image[y:y + h, x:x + w]
                    square_filename = f"{base_name}_square_{i}.jpg"
                    output_path = os.path.join(output_folder, square_filename)
                    cv2.imwrite(output_path, square_img)

                    # –ü—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–µ
                    prediction = self._predict_square(output_path)
                    status = "empty" if prediction == 0 else "marked"
                    
                    new_filename = f"{base_name}_square_{i}_{status}.jpg"
                    new_path = os.path.join(output_folder, new_filename)
                    os.rename(output_path, new_path)

                    # –ò–∑–≤–ª–µ–∫–∞–µ–º –Ω–æ–º–µ—Ä –≤–æ–ø—Ä–æ—Å–∞ –∏–∑ –∏–º–µ–Ω–∏ —Ñ–∞–π–ª–∞ (–Ω–∞–ø—Ä–∏–º–µ—Ä, 1 –∏–∑ Bushueva_czmgvHD_norm_1)
                    question_number = int(base_name.split('_')[-1])
                    
                    if question_number not in results[template_name]:
                        results[template_name][question_number] = {}

                    # –°–æ—Ö—Ä–∞–Ω—è–µ–º —Ç–æ–ª—å–∫–æ marked –æ—Ç–≤–µ—Ç—ã
                    if status == "marked":
                        results[template_name][question_number][i] = status

            except Exception as e:
                logger.error(f"–û—à–∏–±–∫–∞ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –∫–≤–∞–¥—Ä–∞—Ç–æ–≤ –≤ {filename}: {str(e)}")

        # –í—ã–≤–æ–¥–∏–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –≤ –∫–æ–Ω—Å–æ–ª—å
        print("\n–†–µ–∑—É–ª—å—Ç–∞—Ç—ã –æ–±—Ä–∞–±–æ—Ç–∫–∏ —à–∞–±–ª–æ–Ω–æ–≤:")
        for template, questions in results.items():
            print(f"\n–®–∞–±–ª–æ–Ω: {template}")
            for question, answers in sorted(questions.items()):
                print(f"  –í–æ–ø—Ä–æ—Å {question}:")
                for answer, status in sorted(answers.items()):
                    print(f"    –í–∞—Ä–∏–∞–Ω—Ç {answer}: {status}")
        
        return results


    def _predict_square(self, image_path: str) -> int:
        """–ü—Ä–µ–¥—Å–∫–∞–∑—ã–≤–∞–µ—Ç —Å–æ—Å—Ç–æ—è–Ω–∏–µ –∫–≤–∞–¥—Ä–∞—Ç–∞ (0 - –ø—É—Å—Ç–æ–π, 1 - –ø–æ–º–µ—á–µ–Ω–Ω—ã–π)"""
        transform = transforms.Compose([
            transforms.Resize((PredictionConfig.img_size, PredictionConfig.img_size)),
            transforms.ToTensor(),
            transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])
        ])
        
        try:
            image = Image.open(image_path).convert('RGB')
            image = transform(image).unsqueeze(0).to(self.device)

            with torch.no_grad():
                output = self.model(image)
                return (output > 0.5).float().item()
        except Exception as e:
            logger.error(f"–û—à–∏–±–∫–∞ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏—è –¥–ª—è {image_path}: {str(e)}")
            return 0